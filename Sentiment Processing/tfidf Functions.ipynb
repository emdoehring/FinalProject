{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# regular expression import\n",
    "import re\n",
    "\n",
    "# uni-code library\n",
    "import unicodedata\n",
    "\n",
    "# natural language toolkit library/modules\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "def basic_clean(string):\n",
    "    '''\n",
    "    This function takes in a string and\n",
    "    returns the string normalized.\n",
    "    '''\n",
    "    string = unicodedata.normalize('NFKD', string)\\\n",
    "             .encode('ascii', 'ignore')\\\n",
    "             .decode('utf-8', 'ignore')\n",
    "    string = re.sub(r'[^\\w\\s]', '', string).lower()\n",
    "    return string\n",
    "\n",
    "def stem(string):\n",
    "    '''\n",
    "    This function takes in a string and\n",
    "    returns a string with words stemmed.\n",
    "    '''\n",
    "    # Create porter stemmer.\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    \n",
    "    # Use the stemmer to stem each word in the list of words we created by using split.\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    \n",
    "    # Join our lists of words into a string again and assign to a variable.\n",
    "    string = ' '.join(stems)\n",
    "    \n",
    "    return string\n",
    "\n",
    "def lemmatize(string):\n",
    "    '''\n",
    "    This function takes in string for and\n",
    "    returns a string with words lemmatized.\n",
    "    '''\n",
    "    # Create the lemmatizer.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    # Use the lemmatizer on each word in the list of words we created by using split.\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "\n",
    "    # Join our list of words into a string again and assign to a variable.\n",
    "    string = ' '.join(lemmas)\n",
    "    \n",
    "    return string\n",
    "\n",
    "def remove_stopwords(string, extra_words = [], exclude_words = []):\n",
    "    '''\n",
    "    This function takes in a string, optional extra_words and exclude_words parameters\n",
    "    with default empty lists and returns a string.\n",
    "    '''\n",
    "    # Create stopword_list.\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    # Remove 'exclude_words' from stopword_list to keep these in my text.\n",
    "    stopword_list = set(stopword_list) - set(exclude_words)\n",
    "    \n",
    "    # Add in 'extra_words' to stopword_list.\n",
    "    stopword_list = stopword_list.union(set(extra_words))\n",
    "\n",
    "    # Split words in string.\n",
    "    words = word_tokenize(string)\n",
    "    \n",
    "    # Create a list of words from my string with stopwords removed and assign to variable.\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    \n",
    "    # Join words in the list back into strings and assign to a variable.\n",
    "    string_without_stopwords = ' '.join(filtered_words)\n",
    "    \n",
    "    return string_without_stopwords\n",
    "\n",
    "def clean(text):\n",
    "    '''\n",
    "    This function combines the above steps and added extra stop words to clean text\n",
    "    '''\n",
    "    return remove_stopwords(lemmatize(basic_clean(text)), extra_words = ['dont', 'cant', 'im', 'ive', 'ill', 'te', 'youre', 'wan', 'na', 'wa', ''])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDFFromCSV(csvName):\n",
    "    df = pd.read_csv(csvName)\n",
    "    df.dropna(inplace=True)\n",
    "    texts = df['Lyrics']\n",
    "\n",
    "        \n",
    "    cleaned_texts = [clean(text) for text in texts]\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(cleaned_texts)\n",
    "\n",
    "    # Step 3: Get feature names (words)\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Step 4: Get IDF values\n",
    "    idf_values = tfidf_vectorizer.idf_\n",
    "\n",
    "    # Create a dictionary mapping each word to its IDF value\n",
    "    word_idf = dict(zip(feature_names, idf_values))\n",
    "\n",
    "    # Step 5: Generate Word Cloud\n",
    "    # Convert TF-IDF matrix to a dictionary\n",
    "    word_tfidf = dict(zip(feature_names, tfidf_matrix.mean(axis=0).tolist()[0]))\n",
    "    \n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    lyrics_trimmed = []\n",
    "    for key, value in word_tfidf.items():\n",
    "        if value <= 0.001:\n",
    "            lyrics_trimmed.append(key)\n",
    "\n",
    "    trimmed_string = set(lyrics_trimmed)\n",
    "    \n",
    "\n",
    "    filtered_trimmed_lyrics = [word.lower() for word in word_tfidf if word.isalpha() and word.lower() not in stop_words and word.lower() not in trimmed_string]\n",
    "\n",
    "    # Load the sentiment analysis pipeline\n",
    "    sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "    return sentiment_analyzer.polarity_scores(' '.join(filtered_trimmed_lyrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDFFromSong(lyrics):\n",
    "    \n",
    "    texts = lyrics.split()\n",
    "\n",
    "    cleaned_texts = [clean(text) for text in texts]\n",
    "    \n",
    "    if not any(len(s) > 0 for s in cleaned_texts):\n",
    "        return\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(cleaned_texts)\n",
    "\n",
    "    # Step 3: Get feature names (words)\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Step 4: Get IDF values\n",
    "    idf_values = tfidf_vectorizer.idf_\n",
    "\n",
    "    # Create a dictionary mapping each word to its IDF value\n",
    "    word_idf = dict(zip(feature_names, idf_values))\n",
    "\n",
    "    # Step 5: Generate Word Cloud\n",
    "    # Convert TF-IDF matrix to a dictionary\n",
    "    word_tfidf = dict(zip(feature_names, tfidf_matrix.mean(axis=0).tolist()[0]))\n",
    "    \n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    lyrics_trimmed = []\n",
    "    for key, value in word_tfidf.items():\n",
    "        if value <= 0.001:\n",
    "            lyrics_trimmed.append(key)\n",
    "\n",
    "    trimmed_string = set(lyrics_trimmed)\n",
    "    \n",
    "\n",
    "    filtered_trimmed_lyrics = [word.lower() for word in word_tfidf if word.isalpha() and word.lower() not in stop_words and word.lower() not in trimmed_string]\n",
    "\n",
    "    # Load the sentiment analysis pipeline\n",
    "    sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "    return sentiment_analyzer.polarity_scores(' '.join(filtered_trimmed_lyrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDFFromCSVIndividualSongs(CSVName):\n",
    "    df = pd.read_csv(CSVName, index_col=0)\n",
    "    df.dropna(inplace=True)\n",
    "    df[\"Sentiment\"] = df[\"Lyrics\"].apply(TFIDFFromSong)\n",
    "    df.to_csv(CSVName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Danny Brown Lyrics.csv\n",
      "Interpol Lyrics.csv\n",
      "Charli XCX Lyrics.csv\n",
      "The Strokes Lyrics.csv\n",
      "Mac Demarco Lyrics.csv\n",
      "Radiohead Lyrics.csv\n",
      "Jack Johnson Lyrics.csv\n",
      "Sufjan Stevens Lyrics.csv\n",
      "Lorde Lyrics.csv\n",
      "The Decemberists Lyrics.csv\n",
      "The National Lyrics.csv\n",
      "Cage The Elephant Lyrics.csv\n",
      "Neutral Milk Hotel Album Lyrics.csv\n",
      "Mac Miller Lyrics.csv\n",
      "Lana Del Rae Lyrics.csv\n",
      "Silver Jews Lyrics.csv\n",
      "Kacey Musgraves Lyrics.csv\n",
      "Haim Lyrics.csv\n",
      "Pavement Lyrics.csv\n",
      "Lake Street Dive Lyrics.csv\n",
      "The 1975 Lyrics.csv\n",
      "Vampire Weekend Lyrics.csv\n",
      "The Lumineers Lyrics.csv\n",
      "Car Seat Headrest Lyrics.csv\n",
      "Alex G Lyrics.csv\n",
      "Carly Rae Jepson Lyrics.csv\n",
      "Alvvays Lyrics.csv\n",
      "Kendrick Lamar Lyrics.csv\n",
      "Brockhampton Lyrics.csv\n",
      "Chastity Belt Lyrics.csv\n",
      "Big Theif Lyrics.csv\n",
      "Adrianne Lenker Lyrics.csv\n",
      "Frank Ocean Lyrics.csv\n",
      "The Smiths Lyrics.csv\n",
      "Death Grips Lyrics.csv\n",
      "100 Gecs Lyrics.csv\n",
      "Anderson .Paak Lyrics.csv\n",
      "Vampire Weekend Lyrics.csv\n",
      "New Order Lyrics.csv\n"
     ]
    }
   ],
   "source": [
    "allCSVNames = ['Danny Brown Lyrics.csv', 'Interpol Lyrics.csv', 'Charli XCX Lyrics.csv', 'The Strokes Lyrics.csv', 'Mac Demarco Lyrics.csv', 'Radiohead Lyrics.csv', 'Jack Johnson Lyrics.csv', 'Sufjan Stevens Lyrics.csv', 'Lorde Lyrics.csv', 'The Decemberists Lyrics.csv', 'The National Lyrics.csv', 'Cage The Elephant Lyrics.csv', 'Neutral Milk Hotel Album Lyrics.csv', 'Mac Miller Lyrics.csv', 'Lana Del Rae Lyrics.csv', 'Silver Jews Lyrics.csv', 'Kacey Musgraves Lyrics.csv', 'Haim Lyrics.csv', 'Pavement Lyrics.csv', 'Lake Street Dive Lyrics.csv', 'The 1975 Lyrics.csv', 'Vampire Weekend Lyrics.csv', 'The Lumineers Lyrics.csv', 'Car Seat Headrest Lyrics.csv', 'Alex G Lyrics.csv', 'Carly Rae Jepson Lyrics.csv', 'Alvvays Lyrics.csv', 'Kendrick Lamar Lyrics.csv', 'Brockhampton Lyrics.csv', 'Chastity Belt Lyrics.csv', 'Big Theif Lyrics.csv', 'Adrianne Lenker Lyrics.csv', 'Frank Ocean Lyrics.csv', 'The Smiths Lyrics.csv', 'Death Grips Lyrics.csv', '100 Gecs Lyrics.csv', 'Anderson .Paak Lyrics.csv', 'Vampire Weekend Lyrics.csv', 'New Order Lyrics.csv']\n",
    "\n",
    "for i in allCSVNames:\n",
    "    TFIDFFromCSVIndividualSongs(i)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import ast\n",
    "\n",
    "\n",
    "\n",
    "def plot_sentiment_over_time(csv, artist_name):\n",
    "    df = pd.read_csv(csv, index_col=0)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df['Release Date Object'] = df[\"Release Date\"].apply(lambda x: datetime(year=ast.literal_eval(x)['year'], month=ast.literal_eval(x)['month'], day=ast.literal_eval(x)['day']))\n",
    "    df[\"Sentiment\"] = df['Sentiment'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "    df.sort_values(by=\"Release Date Object\")\n",
    "    \n",
    "    positive_sentiments = []\n",
    "    negative_sentiments = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        sentiment = row['Sentiment']\n",
    "        positive_sentiments.append(sentiment['pos'])\n",
    "        negative_sentiments.append(sentiment['neg'])\n",
    "\n",
    "    # Plot the sentiment over time\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(positive_sentiments, label='Positive Sentiment', color='blue')\n",
    "    plt.plot(negative_sentiments, label='Negative Sentiment', color='red')\n",
    "    plt.xlabel('Release Date')\n",
    "    plt.ylabel('Sentiment Score')\n",
    "    plt.title(f'Sentiment Of {artist_name} Lyrics Over Time')\n",
    "    plt.legend()\n",
    "\n",
    "    # Annotate the graph with first and last song names and release dates\n",
    "    first_song_name = df.index[0]\n",
    "    last_song_name = df.index[-1]\n",
    "    first_release_date = df['Release Date Object'].min().strftime(\"%m/%d/%Y\")  # Assuming your index contains release dates\n",
    "    last_release_date = df['Release Date Object'].max().strftime(\"%m/%d/%Y\")\n",
    "\n",
    "    plt.xticks([0, len(df) - 1], [f'{first_song_name}\\n{first_release_date}', f'{last_song_name}\\n{last_release_date}'])\n",
    "\n",
    "    plt.ylim(0.0, 0.8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{artist_name}_Sentiment_Graph_2.0.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in allCSVNames:\n",
    "    tempname = ' '.join(i.split()[:-1])\n",
    "    plot_sentiment_over_time(i, tempname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
